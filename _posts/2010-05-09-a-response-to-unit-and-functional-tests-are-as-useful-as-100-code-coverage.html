---
layout: post
title: A Response to "Unit and Functional Tests Are as Useful as 100% Code Coverage"
date: 2010-05-09 15:47:06.000000000 -06:00
categories: []
tags: []
status: publish
type: post
published: true
meta:
  _edit_last: '2'
  _wp_rp_related_posts_query_result_cache_expiration: '1443763202'
  _wp_rp_related_posts_query_result_cache_4: a:8:{i:0;O:8:"stdClass":2:{s:7:"post_id";s:3:"218";s:5:"score";s:17:"19.18670608096386";}i:1;O:8:"stdClass":2:{s:7:"post_id";s:3:"220";s:5:"score";s:18:"17.788027779030884";}i:2;O:8:"stdClass":2:{s:7:"post_id";s:2:"99";s:5:"score";s:16:"16.6150103599797";}i:3;O:8:"stdClass":2:{s:7:"post_id";s:3:"115";s:5:"score";s:17:"13.71426392453639";}i:4;O:8:"stdClass":2:{s:7:"post_id";s:3:"234";s:5:"score";s:18:"13.622981362637773";}i:5;O:8:"stdClass":2:{s:7:"post_id";s:3:"197";s:5:"score";s:17:"13.47476541845447";}i:6;O:8:"stdClass":2:{s:7:"post_id";s:3:"168";s:5:"score";s:18:"13.300501022912876";}i:7;O:8:"stdClass":2:{s:7:"post_id";s:2:"44";s:5:"score";s:17:"13.25321660728514";}}
author:
  login: pat
  email: patmaddox@me.com
  display_name: pat
  first_name: Pat
  last_name: Maddox
excerpt: !ruby/object:Hpricot::Doc
  options: {}
---
<p>I just finished reading an article by Josh over at Thoughtbot where he called out unit and functional tests as being <a href="http://robots.thoughtbot.com/post/543043259/unit-and-functional-tests-are-as-useful-as-100-code">&#8220;as useful as 100% code coverage.&#8221;</a></p>
<p>My first thought was, &#8220;hrm.  I&#8217;d love to have an app that had 100% code coverage.&#8221;</p>
<p>The goal of his post, as I understand it, is to point out that having a comprehensive suite of unit tests [1] does not guarantee an application&#8217;s correctness, just as having 100% test coverage does not guarantee a codebase&#8217;s correctness.  He&#8217;s absolutely right, of course, but it begs the question, &#8220;How do you guarantee an application&#8217;s correctness?&#8221;</p>
<p>If I had the answer to that I would probably be typing a different blog post from my villa in the Caribbean.</p>
<p>I frequently hear people offer up advice similar to Josh, that while good on the whole, I think is a bit superficial.  I want to expand on those thoughts a bit with some of my own.</p>
<p>First let&#8217;s address the question of whether or not it&#8217;s possible to write a high-quality Rails application <em>without</em> writing acceptance tests.  There is overwhelming evidence that says it is, and I can hold up Thoughtbot as a prime example of a firm that has delighted customers over and over again before Cucumber even existed [2].</p>
<p>That&#8217;s a bit of a straw-man argument, because it&#8217;s &#8220;possible&#8221; to write a high-quality app without writing any sort of automated tests.  I do think it&#8217;s a midget straw-man though, because in my opinion even a team of extremely talented developers will have a difficult time building and maintaining a high-quality app without automated developer tests [3].  On the other hand, that same team, with a good unit test suite and sufficient collaboration with the product owner, has a good chance of shipping a high-quality app even without the aid of automated acceptance tests.</p>
<h1>High quality and correct behavior</h1>
<p>What do we mean when we say that an application is &#8220;high-quality,&#8221; or that it &#8220;behaves correctly?&#8221;  The two concepts are closely related.  In fact, they&#8217;re pretty much the same concept, just with different audiences.</p>
<p>Quality I consider to be an internal property of software, a measure of the stuff under the hood.  The customer cares about quality in the abstract, but to developers it is absolutely critical [3].  Developers spend their time working with the minutiae, the atoms and compounds that make up the software.  All of the new code we write uses existing code, and in order to be effective we need to have a high degree of confidence that the code we&#8217;re using behaves as we expect it to. [4]</p>
<p>An application&#8217;s behavior is ultimately what the customer cares about.  It&#8217;s the set of features that allow a system&#8217;s users to fulfill particular goals or tasks that the users have in mind.  Note that there is an important relationship between application behavior&#8217;s correctness and the internal code quality.  While high-quality code does not ensure that an application behaves correctly, low-quality code pretty much guarantees incorrect application behavior.</p>
<p>Code quality and application behavior thus are closely related.  What&#8217;s the difference?  Let me start by offering this: it&#8217;s possible to write high-quality code that powers a worthless application.</p>
<p>That point is important enough that it bears repeating (feel free to move your eyes up a bit and reread).  I think is what Josh is getting at.  You can put together a team of ridiculously smart programmers, give them a set of requirements, and end up with a beautiful codebase that fulfills the requirements only halfway. [5]  You can have a comprehensive unit test suite that proves there are no logical errors in the code, and be left with code riddled with critical errors in <em>business</em> logic because the developers didn&#8217;t fully understand the business logic in the first place.</p>
<h1>Why write acceptance tests?</h1>
<p>Now it&#8217;s time to address the fundamental question of why write acceptance tests at all?</p>
<p><strong>Executable tests have only one purpose: to define &#8220;done&#8221; in a manner that the customer and developer can agree upon.</strong></p>
<p>That&#8217;s all there is to it.  All the other stuff about making sure that the app is hooked up properly, teasing out requirements from the customer, etc are all incidental.  Just as unit testing is not really about testing [6], neither is acceptance testing.</p>
<p>The best evidence I can give to support this is to examine what happens when developers write acceptance tests with limited customer involvement.  Several times I&#8217;ve tried to write an acceptance test solo, and passed it off to the customer for approval.  I end up receiving little feedback on the test itself, and inevitably I have to do a fair amount of rework to patch up the requirements I missed.</p>
<p>What goes wrong in that scenario?  Is my customer just lazy?  Maybe she just skims the email, mumbles &#8220;yeah okay&#8221; to herself and shoots off a reply before moving on to more important things.  Perhaps.  But I can give her more credit than that.  The real issue though is that we&#8217;ve removed the single most important factor to successfully shipping a product - real communication.</p>
<h1>Acceptance tests are the output from shared learning</h1>
<p>It turns out that acceptance tests have minimal inherent value.  An &#8220;acceptance test&#8221; with no input from the customer is nothing but an <a href="http://jbrains.ca/permalink/299">integrated test</a> written in a weird language.</p>
<p>Acceptance tests cannot take the place of real communication.  If you don&#8217;t believe me, take a story that you have minimal knowledge about, grab your customer, and write an acceptance test together.  You&#8217;ll find that there&#8217;s a lot of back and forth as you figure out what exactly you&#8217;re supposed to build.  It&#8217;s a back and forth that needs to happen, but trying to capture the requirements in executable form in the same instant that you develop an understanding of them is a frustrating exercise. [7]</p>
<p>You need to develop a deep understanding of the requirements before you can begin to work on acceptance tests.  That comes through conversations with your customer, through white board design sessions with domain experts, through actual human interaction.  Once you&#8217;ve acquired an understanding of the requirements, you can sit down together and write acceptance tests.  Here you&#8217;ll get into another round of back and forth, but this will be to tease out the subtleties of the requirements, rather than understanding the major aspects. [8]</p>
<h1>Maximize the quality of your communication</h1>
<p>The old adage &#8220;two heads are better than one&#8221; hints at the benefits of pairing developers with customers to write acceptance tests.  Sometimes when I don&#8217;t fully understand a requirement, it feels easier to take my best guess at it and soldier onward.  This works when I&#8217;ve been on the project for a long time and have a strong vision of what I&#8217;m building, but often it just feels like a safe decision in the short-term.  It feels better in that moment to try something and get it wrong and iterate, than it does to admit that I don&#8217;t fully understand a requirement.  That&#8217;s a bad trap to fall into, because when done repeatedly, it accumulates needless added risk to the project.</p>
<p>All it takes to clear up a foggy understanding is a question.  That&#8217;s what I like most about pairing with customers - I don&#8217;t have to spend many brain cycles figuring out what I need to build.  When I have uncertainty, I can just ask a question and the ball is back in my customer&#8217;s court.</p>
<h1>No panacea</h1>
<p>It&#8217;s important to remember that no single practice can deliver everything you need to run a successful software project.  That&#8217;s why I think it&#8217;s damaging to make certain kinds of comparisons.  It&#8217;s too easy to throw the baby out with the bath water.  Instead, recognize that the different practices are supportive of each other.  Experiment and see where you get the most bang for your buck, and shift your focus based on that so you get the maximum value possible for your project and team.</p>
<hr />
<p>[1] I&#8217;m going to avoid using &#8220;functional tests&#8221; in this discussion.  &#8220;Functional test&#8221; had an established meaning in the rest of the industry long before Rails hijacked the term, and it&#8217;s synonymous with what we and the rest of the industry now call &#8220;acceptance tests.&#8221;</p>
<p>[2] Cucumber didn&#8217;t invent acceptance tests, of course.  And Thoughtbot could easily have been using something like FIT to do automated ATs before they had Cucumber available, but I&#8217;m gonna gamble and say they didn&#8217;t.</p>
<p>[3] Okay, to <em>some</em> developers it&#8217;s absolutely critical :)</p>
<p>[4] Perhaps that sheds some light on why there&#8217;s so much procedural code out there.  Programmers know to a near certainty that the language&#8217;s built-in library works, so they prefer to use that rather than build their own abstractions.</p>
<p>[5] Actually if you put programmers in a room and give them a bunch of requirements you&#8217;re pretty much guaranteed to end up with a program that fails to deliver on the requirements.  Thus begat Agile.</p>
<p>[6] The benefit of TDD isn&#8217;t primarily in the act of verification.  It&#8217;s that it produces code that is so simple that it&#8217;s difficult to break.  When I use code that has been designed artfully, I can make reasonable assumptions about how to use and modify it.  The automated test suite verifies those assumptions.</p>
<p>[7] I participated in a workshop once where we were given an exercise to incrementally build a billing application for a veterinarian&#8217;s office.  The requirements were written in narrative form.  As I read the narrative aloud to my pair, he told me to slow down.  He was transcribing every sentence I read into a line of code.  I was flabbergasted.  It&#8217;s an interesting approach to building pseudocode, and I&#8217;m not going to discount it as useless, but to write the pseudocode before even having read the full story is insane!!</p>
<p>[8] By the way, the necessary back and forth is the reason why asynchronous acceptance test writing is so ineffective.  If I&#8217;m sitting alongside my customer, we can resolve a whole host of issues in a matter of minutes.  Trying to do the same via email introduces so much friction that many important questions go unasked.</p>
